---
title: "crime_weather_analysis.Rmd"
author: "Sasikumar Venkatesan "
date: " "
output:
  pdf_document:
    latex_engine: xelatex
    fig_width: 6.5
    fig_height: 3.5
    fig_caption: yes
  html_document:
    df_print: paged
  word_document: default
---

```{r setup, include=FALSE}
# Clean the R environment
rm(list = ls()) 

# Set global chunk options
knitr::opts_chunk$set(echo = FALSE, include = FALSE, 
                     warning = FALSE, message = FALSE,
                     fig.pos = "h", out.width = "100%")

# Load required packages

library(dplyr)         # For data manipulation
library(knitr)         # For tables
library(kableExtra)    # For enhanced tables


```

\Large \underline{\textbf{1. Data exploration}}

```{r}
# Load the dataset from the CSV file
mydata <- read.csv("MA334-SP-7_2411729.csv")

# Count the number of observations (rows)
n_obs <- nrow(mydata)  # Total number of individuals in the dataset

# Count the number of variables (columns)
n_vars <- ncol(mydata)  # Total number of measured variables

# View the structure of the dataset (names, types, and first values)
str(mydata)  # Shows each variable's name and data type (int, factor, char, etc.)

```
### 1.1 Dataset Overview
\small The dataset consists of `r n_obs` observations across `r n_vars` variables.  
The variables include **demographic characteristics** (age, gender, race, marital status, number of children), **economic factors** (wage, education level, hours worked), and **employment characteristics** (insurance status, union membership, metropolitan status, region).

The **numeric variables** include age, hours worked per week (`hrswork`), number of children (`nchild`), and hourly wage (`wage`).  
The **categorical variables** include education level (`educ`), gender, insurance status, metropolitan status, union membership, race, marital status, and geographic region.

### 1.2a Descriptive Statistics

```{r numeric-summary, include=TRUE}
# Define truly numeric variables
vars <- c("age", "hrswork", "nchild", "wage")

# Create summary table
tab <- data.frame(
  Variable = vars,
  Min = sapply(mydata[vars], min),
  Q1 = sapply(mydata[vars], function(x) quantile(x, 0.25)),
  Median = sapply(mydata[vars], median),
  Mean = sapply(mydata[vars], mean),
  Q3 = sapply(mydata[vars], function(x) quantile(x, 0.75)),
  Max = sapply(mydata[vars], max),
  `Std Dev` = sapply(mydata[vars], sd)
)

# Round numeric columns
tab[, 2:8] <- round(tab[, 2:8], 2)

# Display table
kable(tab, format = "latex", row.names = FALSE,
      caption = "Summary statistics for numerical variables.") %>%
  kable_styling(font_size = 8, latex_options = "HOLD_position")
```

\small This table presents summary statistics for four key numerical variables:

**Age**: Averaging `r round(mean(mydata$age), 1)` years (range: `r min(mydata$age)`-`r max(mydata$age)`), the dataset covers a wide range of working-age individuals. **Hours Worked**: Typically `r median(mydata$hrswork)` hours weekly, with some reporting up to `r max(mydata$hrswork)` hours. **Number of Children**: Mean `r round(mean(mydata$nchild), 2)` with median `r median(mydata$nchild)`, indicating most have few or no children. **Wage**: The right-skewed distribution shows mean \$`r round(mean(mydata$wage), 2)` exceeding median \$`r median(mydata$wage)`, with maximum \$`r max(mydata$wage)`.

### 1.2b Categorical Summary

```{r categorical-summary, include=TRUE}

# Create efficient categorical summary including education
cat_summary <- data.frame(
  Variable = c("Gender", "", 
               "Marital", "", "", 
               "Insurance", "", 
               "Union", "", 
               "Education", "", "", "", "", "", 
               "Race", "", "", 
               "Region", "", "", ""),
  Category = c("0 (Male)", "1 (Female)", 
               "0 (Single)", "1 (Married)", "2 (Divorced)",
               "0 (No)", "1 (Yes)",
               "0 (No)", "1 (Yes)",
               "0 (High school)", "1 (College no degree)", "2 (College with degree)",
               "3 (BA)", "4 (MA)", "5 (PhD)",
               "Asian", "Black", "White",
               "midwest", "northeast", "south", "west"),
  Count = c(
    as.vector(table(mydata$gender)), 
    as.vector(table(mydata$marital)), 
    as.vector(table(mydata$insure)),
    as.vector(table(mydata$union)),
    as.vector(table(factor(mydata$educ, levels = 0:5))),
    as.vector(table(mydata$race)),
    as.vector(table(mydata$region))
  )
)

# Display the table
kable(cat_summary, format = "latex", row.names = FALSE,
      caption = "Distribution of categorical variables.", 
      col.names = c("Variable", "Category", "Count")) %>%
  kable_styling(font_size = 6.5)

```

\small The dataset includes `r table(mydata$gender)[1]` males (0) and `r table(mydata$gender)[2]` females (1). Most participants are married (`r table(mydata$marital)[2]`), with others single (`r table(mydata$marital)[1]`) or divorced (`r table(mydata$marital)[3]`).

Education levels include:
- High school (`r table(mydata$educ)["0"]`),
- College without degree (`r table(mydata$educ)["1"]`),
- College with degree (`r table(mydata$educ)["2"]`),
- Bachelor's degree (`r table(mydata$educ)["3"]`),
- Master's degree (`r table(mydata$educ)["4"]`), and
- PhD (`r table(mydata$educ)["5"]`).

For insurance, `r table(mydata$insure)[2]` have coverage and `r table(mydata$insure)[1]` do not. Similarly, `r table(mydata$union)[1]` are non-union and `r table(mydata$union)[2]` are union members. Most (`r table(mydata$metro)[2]`) live in metropolitan areas. The sample is predominantly White (`r table(mydata$race)["White"]`), with smaller Black (`r table(mydata$race)["Black"]`) and Asian (`r table(mydata$race)["Asian"]`) groups. Regionally, South (`r table(mydata$region)["south"]`) has the highest representation, followed by West (`r table(mydata$region)["west"]`), Midwest (`r table(mydata$region)["midwest"]`), and Northeast (`r table(mydata$region)["northeast"]`).


### 1.3 Visualising Distributions

```{r combined-figure, fig.width=7.5, fig.height=5, include=TRUE}
# Create multipanel plot for key variable distributions
par(mfrow = c(2, 3), mar = c(4, 4, 2, 1), oma = c(0, 0, 3, 0))

# Wage distribution (numeric - histogram)
hist(mydata$wage,
     main = "Hourly Wage",
     col = "steelblue",
     border = "white",
     xlab = "Wage ($)",
     breaks = 20)

# Education distribution (categorical - barplot)
barplot(table(factor(mydata$educ, levels = 0:5)),
        main = "Education Level",
        col = "coral",
        xlab = "Level (0--5)",
        names.arg = c("0", "1", "2", "3", "4", "5"))

# Hours worked per week (numeric - histogram)
hist(mydata$hrswork,
     main = "Hours Worked",
     col = "darkgreen",
     border = "white",
     xlab = "Weekly Hours",
     breaks = 15)

# Race distribution (categorical - barplot)
barplot(table(mydata$race),
        main = "Race",
        col = "orchid",
        xlab = "Category")

# Gender distribution (categorical - barplot)
barplot(table(factor(mydata$gender, levels = c(0, 1), labels = c("Male", "Female"))),
        main = "Gender",
        col = "goldenrod",
        xlab = "")

# Number of children (numeric - barplot)
barplot(table(mydata$nchild),
        main = "Number of Children",
        col = "lightblue",
        xlab = "Children")

# Overall figure title
mtext("Figure 1: Distributions of Key Variables", side = 3, line = 0, outer = TRUE, font = 2)
```
Figure 1 presents distributions of key variables:

- **Wage**: Right-skewed, with most individuals earning \$10-\$30 hourly and fewer earning higher amounts.
- **Education**: Most respondents have education levels 0-2, representing high school (0), college without degree (1), and college with degree (2), with fewer holding advanced degrees.
- **Hours Worked**: Concentrated around 40-45 hours weekly, demonstrating the prevalence of standard full-time employment.
- **Race**: Predominantly White, with smaller proportions of Black and Asian participants reflecting the sample demographics.
- **Gender**: A balanced distribution with slightly more males than females.
- **Children**: Most individuals have 0-2 children, with frequency decreasing as number of children increases.

### 1.4 Correlation Analysis

```{r correlation-heatmap, fig.height=2.0, fig.width=5, include=TRUE}
library(reshape2)
library(ggplot2)

# Define numerical variables
vars <- c("age", "hrswork", "nchild", "wage")
num_vars <- mydata[vars]

# Correlation matrix
corr_mat <- round(cor(num_vars, use = "complete.obs"), 2)
melted_corr <- melt(corr_mat)

# Blue color gradient heatmap
ggplot(melted_corr, aes(x = Var1, y = Var2, fill = abs(value))) +  # use absolute value
  geom_tile(color = "white", linewidth = 0.3) +
  geom_text(aes(label = value), size = 3, color = "black") +
  scale_fill_gradient(
    low = "#deebf7",  # light blue
    high = "#08519c", # dark blue
    name = "Strength"
  ) +
  theme_minimal() +
  labs(
    title = "Figure 2: Correlation Strength",
    x = "", y = ""
  ) +
  theme(
    legend.position = "right",
    plot.title = element_text(size = 11, face = "bold"),
    axis.text = element_text(face = "bold"),
    axis.ticks = element_blank(),
    panel.grid = element_blank(),
    panel.border = element_blank()
  )


```

\small Figure 2 summarizes correlations among key numerical variables. Age and wage show a moderate positive correlation (`r cor(mydata$age, mydata$wage, use = "complete.obs") %>% round(2)`), indicating earnings tend to rise with experience, consistent with human capital theory. Hours worked and wage are weakly correlated (`r cor(mydata$hrswork, mydata$wage, use = "complete.obs") %>% round(2)`), suggesting time worked does not strongly predict wage levels.

Age and number of children are positively correlated (`r cor(mydata$age, mydata$nchild, use = "complete.obs") %>% round(2)`), reflecting life stage progression. Wage and number of children show a weak association (`r cor(mydata$wage, mydata$nchild, use = "complete.obs") %>% round(2)`), implying limited income impact from family size. Hours worked and number of children show negligible correlation (`r cor(mydata$hrswork, mydata$nchild, use = "complete.obs") %>% round(2)`), indicating diverse work-family patterns.

Overall, most correlations are modest, with no strong negative relationships, suggesting low multicollinearity risk for regression modeling.

\Large \underline{\textbf{2. Probability, Probability Distributions and Confidence Intervals}}

```{r probability-calculations, include=FALSE}
# 1. Probability that at least 1 of 5 is not insured
p_insured <- mean(mydata$insure == 1)
p_not_insured <- 1 - p_insured
p_at_least_one_not_insured <- 1 - (p_insured)^5
# 2. P(nchild ≥ 1 | married)
p_nchild_given_married <- mean(mydata$nchild[mydata$marital == 1] >= 1)
# 3. Distribution of nchild
nchild_dist <- table(mydata$nchild) / nrow(mydata)
nchild_table <- data.frame(
  nchild = as.integer(names(nchild_dist)),
  `P(nchild)` = round(as.vector(nchild_dist), 4)
)
# Calculate mean, variance, and P(nchild ≥ 3)
mean_nchild <- sum(as.integer(names(nchild_dist)) * as.vector(nchild_dist))
var_nchild <- sum((as.integer(names(nchild_dist)) - mean_nchild)^2 * as.vector(nchild_dist))
p_nchild_3_or_more <- sum(nchild_dist[as.integer(names(nchild_dist)) >= 3])
```

\small This section presents key probabilities and distributions derived from the dataset:

1. **Insurance coverage probability**: The probability that a randomly selected individual is not covered by private health insurance is `r round(p_not_insured, 3)`. The probability that **at least one** out of five randomly selected individuals is not insured is:
\[
P(\text{at least one not insured}) = 1 - P(\text{all five are insured}) = 1 - (p_{insured})^5 = 1 - (`r round(p_insured, 3)`)^5 = \mathbf{`r round(p_at_least_one_not_insured, 3)`}
\]
This high probability (`r round(p_at_least_one_not_insured*100, 1)`%) reflects significant gaps in private insurance coverage across the population. This has implications for healthcare accessibility and financial vulnerability, suggesting many households rely on public options or remain uninsured.

2. **Conditional probability**: The probability that a person selected from married individuals has one or more children is:
\[
P(nchild \geq 1 \mid \text{married}) = \mathbf{`r round(p_nchild_given_married, 3)`}
\]
This result demonstrates that marriage and parenthood remain strongly associated, though approximately `r round((1-p_nchild_given_married)*100, 1)`% of married individuals have no children. This substantial childless married segment challenges traditional assumptions about family formation and may reflect changing socioeconomic priorities or delayed childbearing among couples.

3. **Number of children distribution**: The probability distribution reveals most individuals have no children, with a **mean** of `r round(mean_nchild, 2)` children per person and **variance** of `r round(var_nchild, 2)`.

```{r probability-calculation, include=TRUE}
kable(nchild_table, format = "latex", 
      caption = "Probability distribution of children (nchild).",
      col.names = c("nchild", "P(nchild)")) %>%
  kable_styling(latex_options = c("HOLD_position"), full_width = FALSE)
```
The probability of having three or more children is:
\[
P(nchild \geq 3) = \mathbf{`r round(p_nchild_3_or_more, 3)`}
\]
This relatively low probability of larger families (`r round(p_nchild_3_or_more*100, 1)`%) reflects the demographic transition toward smaller family sizes in developed economies. The mean below replacement level (2.1 children) suggests population decline without immigration, consistent with broader demographic challenges facing many advanced economies with aging populations and declining birth rates.


\Large \underline{\textbf{3. Point Estimates, Confidence Intervals \& Hypothesis Tests}}

```{r confidence-calculations, include=FALSE}
# 1. Households with exactly 2 children
wage_2child <- mydata[mydata$nchild == 2, "wage"]
mean_2child <- mean(wage_2child, na.rm = TRUE)
ci_2child <- t.test(wage_2child)$conf.int

# 2. Households with 5 or more children
wage_5plus <- mydata[mydata$nchild >= 5, "wage"]
mean_5plus <- mean(wage_5plus, na.rm = TRUE)
ci_5plus <- if (length(na.omit(wage_5plus)) > 1) t.test(wage_5plus)$conf.int else NULL

# 3. Chi-square test: insurance by gender
insurance_gender <- table(Gender = mydata$gender, Insurance = mydata$insure)
chi_test <- chisq.test(insurance_gender)
```

\small 
### 1. Wage Estimate for Households with 2 Children

\small The average wage for individuals in households with exactly 2 children is **$`r round(mean_2child, 2)`**.  
The 95% confidence interval for the population mean wage is:**[`r round(ci_2child[1], 2)`, `r round(ci_2child[2], 2)`]**. This interval suggests that with 95% confidence, the true average wage for households with two children falls within the range above. The relatively narrow interval reflects a good sample size and reliable estimation.

### 2. Wage Estimate for Households with 5 or More Children

The average wage for individuals in households with 5 or more children is **$`r round(mean_5plus, 2)`**.

```{r ci-5plus-interpretation, echo=FALSE, results='asis'}
if (!is.null(ci_5plus)) {
  cat(paste0("The 95\\% confidence interval is: **[", 
             round(ci_5plus[1], 2), ", ", round(ci_5plus[2], 2), "]**."))
} else {
  cat("A 95\\% confidence interval could not be calculated due to insufficient sample size.")
}
```

Because this subgroup contains fewer observations, the confidence interval may be wide or unavailable. Therefore, the statistical reliability of this estimate is limited, and conclusions should be drawn with caution.

### 3. Testing Independence of Insurance Status and Gender

We investigate whether **insurance status** is statistically associated with **gender** using a chi-square test of independence.

\small
- **Null Hypothesis (H\textsubscript{0})**: Insurance status and gender are independent  
- **Alternative Hypothesis (H\textsubscript{1})**: Insurance status and gender are not independent

```{r insurance-table-rmd, include=TRUE}
kable(insurance_gender, format = "latex",
      caption = "Contingency table: Insurance status by gender.",
      col.names = c("Uninsured", "Insured")) %>%
  kable_styling(latex_options = c("HOLD_position", "striped", "bordered", "scale_down"), full_width = FALSE, font_size = 9)
```

The chi-square test returned the following statistics:
- \(\chi^2 = \) `r round(chi_test$statistic, 3)`  
- Degrees of freedom = `r chi_test$parameter`  
- p-value = `r format.pval(chi_test$p.value, digits = 3)`

Using a 5% significance level,  
`r if (chi_test$p.value < 0.05) {
  "we reject the null hypothesis, indicating that insurance status and gender are statistically associated."
} else {
  "we fail to reject the null hypothesis, suggesting no significant evidence of an association between gender and insurance status."
}`

\Large \underline{\textbf{4. Simple Linear Regression}}

```{r setup-simple-regression, include=FALSE}
# Prepare data
mydata$log_wage <- log(mydata$wage)
young <- subset(mydata, age < 35)
old <- subset(mydata, age >= 35)

# Fit models
model_young <- lm(log_wage ~ age, data = young)
model_old <- lm(log_wage ~ age, data = old)

# Extract coefficients and model statistics
coef_young <- coef(model_young)
coef_old <- coef(model_old)
r2_young <- summary(model_young)$r.squared
r2_old <- summary(model_old)$r.squared
young_p <- summary(model_young)$coefficients[2, 4]
old_p <- summary(model_old)$coefficients[2, 4]
```
\small

This section examines how age influences wages for different age groups using simple linear regression models.

### 4.1 Model Specification

For both age groups, we specify a semi-logarithmic model:
\[
\log(\text{wage}) = \beta_0 + \beta_1 \cdot \text{age} + \varepsilon
\]
This transformation allows us to interpret the coefficient \(\beta_1\) as the approximate **percentage change in wage** associated with a one-year increase in age.

#### Results for Young Workers (Age < 35)

The fitted model for younger workers is:
\[
\log(\text{wage}) = `r round(coef_young[1], 3)` + `r round(coef_young[2], 3)` \cdot \text{age}
\]
Each additional year of age is associated with approximately a **`r round(coef_young[2]*100, 1)`%** increase in hourly wage (p-value = `r format.pval(young_p, digits = 3)`). The coefficient of determination \( R^2 = `r round(r2_young, 3)` \) indicates that age explains **`r round(r2_young*100, 1)`%** of the variation in log wages among younger workers. This suggests that **early career progression** meaningfully correlates with wage growth.

#### Results for Older Workers (Age >= 35)

The fitted model for older workers is:
\[
\log(\text{wage}) = `r round(coef_old[1], 3)` + `r round(coef_old[2], 5)` \cdot \text{age}
\]
The age coefficient is **nearly zero** and not statistically significant (p-value = `r format.pval(old_p, digits = 3)`). The extremely low \( R^2 = `r round(r2_old, 5)` \) reveals that age explains virtually **none** of the wage variation in this group. This indicates that **wage growth essentially plateaus** after age 35.

### 4.2 Visualization and Comparison
```{r plot-simple-regression, fig.width=7.2, fig.height=2.5, include=TRUE}
par(mfrow = c(1, 2), mar = c(4, 4, 3, 1))

# Young group plot
plot(young$age, young$log_wage,
     main = "Young Workers (Age < 35)",
     xlab = "Age", ylab = "log(Wage)",
     col = "steelblue3", pch = 19)
abline(model_young, col = "navy", lwd = 2)
text(min(young$age) + 1, max(young$log_wage) - 0.2,
     paste("R² =", round(r2_young, 3)), pos = 4, cex = 0.7)

# Old group plot
plot(old$age, old$log_wage,
     main = "Older Workers (Age >= 35)",
     xlab = "Age", ylab = "log(Wage)",
     col = "indianred3", pch = 20)
abline(model_old, col = "darkred", lwd = 2)
text(min(old$age) + 1, max(old$log_wage) - 0.2,
     paste("R² =", round(r2_old, 5)), pos = 4, cex = 0.7)
```

The scatter plots clearly illustrate the contrasting relationships between age and log wage. **Young workers display a noticeable upward trend**, indicating that wages tend to increase with age during early career stages. The **moderate \( R^2 \)** value suggests that age is a useful predictor in this group, though other factors likely contribute as well. In contrast, **older workers show a nearly flat pattern**, with data points widely dispersed around the horizontal regression line. The **negligible \( R^2 \)** value confirms that age has little explanatory power for wages among older individuals.

### Interpretation of Findings

These results reveal an important labor market pattern: **wage growth is concentrated in early career stages** and stabilizes after mid-career. The stark difference in coefficients of determination between the two groups suggests that **human capital accumulation in early years drives wage increases**, whereas **beyond age 35, other variables such as skills, industry, education, and individual career paths become more relevant**. The **simple linear model is thus more appropriate for younger workers**, while explaining wage variation in older groups requires a **more complex, multivariate approach**. This supports the established economic view that the relationship between age and earnings is **non-linear**, with **diminishing returns to experience** over time.


\Large \underline{\textbf{5. Multiple Linear Regression}}

```{r setup-multiple-regression, include=FALSE}
# Convert categorical variables to factors
mydata_factor <- mydata
cat_vars <- c("gender", "marital", "race", "region", "insure", "union", "metro", "educ")
mydata_factor[cat_vars] <- lapply(mydata_factor[cat_vars], factor)

# Subset data
young <- subset(mydata_factor, age < 35)
old <- subset(mydata_factor, age >= 35)

# Fit full multiple linear regression models
multi_young <- lm(log_wage ~ . -wage -log_wage, data = young)
multi_old <- lm(log_wage ~ . -wage -log_wage, data = old)

# Extract model summary statistics
young_summary <- summary(multi_young)
old_summary <- summary(multi_old)
adjr2_young <- young_summary$adj.r.squared
adjr2_old <- old_summary$adj.r.squared

# Get significant predictors
young_sig <- young_summary$coefficients[,4] < 0.05
old_sig <- old_summary$coefficients[,4] < 0.05

# Count significant predictors
young_sig_count <- sum(young_sig) - 1  # Subtract 1 for intercept
old_sig_count <- sum(old_sig) - 1  # Subtract 1 for intercept
```

## 5.1 Model Setup and Specification

\small To examine how multiple factors jointly affect wages at different career stages, I fit separate multiple linear regression models for young (age < 35) and old (age >= 35) individuals, predicting `log(wage)` using all available explanatory variables. **Categorical variables** (`gender`, `marital status`, `race`, `region`, `insurance`, `union`, `metro`, and `education`) were properly converted to factors, allowing R to automatically create appropriate dummy variables with reference categories. This transformation is essential for correctly interpreting categorical effects in regression analysis. The comprehensive model equation is: 

\[
\log(\text{wage}) = \beta_0 + \beta_1 \cdot \text{age} + \beta_2 \cdot \text{hrswork} + \beta_3 \cdot \text{nchild} + \sum_{i=4}^{p} \beta_i \cdot X_i + \varepsilon
\]

Where \( X_i \) represents the dummy variables created from categorical predictors, and \( p \) is the total number of coefficients.


```{r include=TRUE,fig.width=7, fig.height=4}
par(mfrow = c(2, 2), mar = c(4, 4, 3, 1))
# Young group: Residuals vs Fitted
plot(multi_young$fitted.values, multi_young$residuals,
     main = "Residuals vs Fitted (Young)",
     xlab = "Fitted Values", ylab = "Residuals",
     pch = 20, col = "thistle")
abline(h = 0, col = "purple", lwd = 1.5)

# Young group: Q-Q Plot
qqnorm(multi_young$residuals, main = "Q-Q Plot (Young)", pch = 20, col = "thistle")
qqline(multi_young$residuals, col = "purple", lwd = 1.5)

# Old group: Residuals vs Fitted
plot(multi_old$fitted.values, multi_old$residuals,
     main = "Residuals vs Fitted (Old)",
     xlab = "Fitted Values", ylab = "Residuals",
     pch = 20, col = "lightcoral")
abline(h = 0, col = "red", lwd = 1.5)

# Old group: Q-Q Plot
qqnorm(multi_old$residuals, main = "Q-Q Plot (Old)", pch = 20, col = "lightcoral")
qqline(multi_old$residuals, col = "red", lwd = 1.5)
```
The diagnostic plots show generally acceptable model fit for both age groups.  
The **residuals vs fitted** plots reveal relatively random scatter around zero, indicating no major issues with heteroscedasticity or non-linearity.  
The **Q-Q plots** show reasonable normality in the central regions with some deviation in the tails, which is common in wage data due to outliers at the upper end of the distribution.

## 5.2 Model Results and Comparison
```{r include=TRUE}
# Display key statistics from both models
model_stats <- data.frame(
  Statistic = c("Adjusted R²", "F-statistic", "p-value", "Significant predictors"),
  Young = c(
    round(young_summary$adj.r.squared, 3),
    round(young_summary$fstatistic[1], 2),
    format.pval(pf(young_summary$fstatistic[1], young_summary$fstatistic[2], 
                  young_summary$fstatistic[3], lower.tail = FALSE), digits = 3),
    young_sig_count
  ),
  Old = c(
    round(old_summary$adj.r.squared, 3),
    round(old_summary$fstatistic[1], 2),
    format.pval(pf(old_summary$fstatistic[1], old_summary$fstatistic[2], 
                  old_summary$fstatistic[3], lower.tail = FALSE), digits = 3),
    old_sig_count
  )
)
kable(model_stats, caption = "Summary statistics for multiple regression models")
```

\small For **young workers (age < 35)**, the multiple regression model achieved an adjusted R² of `r round(adjr2_young, 3)`, a significant improvement from the simple model’s `r round(r2_young, 3)`.  
The most influential predictors were **education level**, **union membership**, and **gender**. Age remained a significant predictor even after adjusting for these variables, supporting the idea that younger individuals tend to experience wage growth as they gain experience and skills early in their careers.

For **older workers (age >= 35)**, the adjusted R² improved dramatically to `r round(adjr2_old, 3)`, up from a near-zero R² of `r round(r2_old, 5)` in the simple model.  
In this group, **education**, **region**, and **insurance status** were more predictive of wage than age. Age itself became statistically insignificant, reinforcing the conclusion that wage growth tends to plateau after mid-career.

This contrast between models confirms that while **age matters early in a career**, it becomes a much less useful predictor later in life. A richer set of variables is necessary to explain wage variation for older individuals, making the full model far more effective for that group.


## 5.3 Benefits of Reduced Models

\small While the full models capture a broad range of variables, they may include predictors that contribute little to overall explanatory power. A **reduced model** with only statistically significant predictors offers several practical advantages:

**Clarity and interpretability**: Models with fewer variables are easier to explain to stakeholders and more transparent in identifying key wage drivers such as education or union membership. **Reduced risk of overfitting**: Full models, particularly with many categorical variables (e.g., race, region), may fit noise in the data rather than meaningful patterns. A reduced model helps generalize better to new samples. **Increased model stability**: Removing weak or correlated predictors can reduce variance in coefficient estimates, improving consistency across datasets.
**Less multicollinearity**: Highly correlated predictors, such as education and region, can inflate standard errors and obscure true relationships. A reduced model helps control for this. **Greater computational efficiency**: Simpler models are easier to implement, visualize, and interpret in practice, particularly when used in predictive applications or interactive dashboards.

A reduced model can be obtained through **backward elimination**, **stepwise regression**, or using **AIC/BIC** criteria. This approach allows us to retain only the most meaningful predictors while still capturing the key variation in wages, balancing complexity with predictive accuracy.

###  Conclusion

\small The multiple regression analysis provided a more complete understanding of wage determinants compared to the simple age-based models.  

For **younger individuals**, age remained a strong predictor of wages, along with education and union membership — reflecting early-career wage growth patterns.  
For **older individuals**, the predictive power of age diminished, while **education**, **region**, and **insurance status** played larger roles in explaining wage differences. The substantial improvement in adjusted R² from simple to full models — particularly for the older group — underscores the **importance of multivariable approaches** when analyzing labor income data. This analysis highlights the **complex and evolving nature of wage determination** across career stages, showing that a one-size-fits-all model (like age alone) is insufficient for explaining earnings across the population. Overall, this investigation demonstrates the value of statistical modeling in uncovering how different demographic, educational, and employment factors shape income patterns in the labor market.

